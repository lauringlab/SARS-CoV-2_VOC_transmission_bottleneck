### Author: Andrew Valesano
### Purpose: Get consensus genomes from Illumina sequencing data.
### This is designed for using the ARTIC primers for SARS-CoV-2 sequenced on Illumina.

# ============================= How to run this pipeline ==========================

# 1. Modify the parameters below as needed ("rule parameters").
# 2. Load modules: module load Bioinformatics ivar python2.7-anaconda/2019.03 samtools/1.9 fastqc picard-tools bwa bedtools2 R
# 3. Copy fastq files to data/fastq.
# 4. Rename raw fastq files: python ~/variant_pipeline_resources/change_miseq_names_sars2.py -s data/fastq -f data/fastq_renamed -run
# 5. Unzip fastq files: gunzip -v data/fastq_renamed/*.gz
# 6. Activate snakemake: conda activate snakemake
# 7. Run job on Slurm: sbatch submit_snakemake.sbat -- Or run directly: snakemake -s Snakefile-BWA -p --latency-wait 30 --cores 2

# ============================= Configure run options here =============================

IDS, = glob_wildcards("data/fastq_renamed/{id}_A.1.fastq.gz") # Where the pipeline will grab all of the IDs to run. Important to have changed the filenames first.

rule all:
    input:
        "all.consensus.fasta",
        expand ("data/ivar_output/coverage/{id}_1.coverage.csv", id=IDS)

rule parameters:
    params:
        bed_file = "ncov_references/SARS-CoV-2_V4.1_primers_alt_overlap_sites.bed",
        reference_fasta = "ncov_references/bwa_ref/nCov_WH1_ref.fasta", # fasta used for alignment
        reference_index = "ncov_references/bwa_ref/WH1", # bwa index used for alignment. Should be a build of reference_fasta
        min_length = 29000, # minimum length of consensus genomes in final fasta file
        name = "run", # Goes into the coverage.csv output file for tracking
        min_qual_score = 0, # minimum quality score used in iVar consensus. Important that this is zero for calling indels.
        consensus_threshold = 0, # frequency threshold value used in iVar consensus. See documentation.
        min_depth = 10, # minimum depth used in iVar consensus
        cutadapt_seq_fwd = "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA", # sequence used for adapter trimming. This is NEBnext (same as TruSeq). Nextera adapter sequence, forward and reverse: CTGTCTCTTATACACATCT
        cutadapt_seq_rev = "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT",
        bowtie_option = "--very-sensitive-local", # bowtie2 mapping option
        human_ref = "/nfs/turbo/med-alauring2/avalesan/human_ref/hr38" # path to the human reference genome for filtering

setup = rules.parameters.params

# ============================= Here are the pipeline rules =============================

rule fastqc:
    message:
        """
        =======================================================
        Run FastQC
        =======================================================
        """
    input:
        reads_1_in = "data/fastq_renamed/{id}_A.1.fastq.gz",
        reads_2_in = "data/fastq_renamed/{id}_A.2.fastq.gz",
        reads_3_in = "data/fastq_renamed/{id}_B.1.fastq.gz",
        reads_4_in = "data/fastq_renamed/{id}_B.2.fastq.gz"

    output:
        output1 ="data/aligned_output/fastqc/{id}_1.1_fastqc.zip",
        output2="data/aligned_output/fastqc/{id}_1.2_fastqc.zip",
        output3 ="data/aligned_output/fastqc/{id}_2.1_fastqc.zip",
        output4="data/aligned_output/fastqc/{id}_2.2_fastqc.zip"
    params:
    	"data/aligned_output/fastqc"
    run:
        shell("fastqc -o {params} --noextract -f fastq {input.reads_1_in}")
        shell("fastqc -o {params} --noextract -f fastq {input.reads_2_in}")
        shell("fastqc -o {params} --noextract -f fastq {input.reads_3_in}")
        shell("fastqc -o {params} --noextract -f fastq {input.reads_4_in}")

rule bwa_align:
    message:
        """
        =======================================================
        Map with BWA and sort
        =======================================================
        """
    input:
        reads_1_in = "data/fastq_renamed/{id}_1.1.fastq.gz",
        reads_2_in = "data/fastq_renamed/{id}_1.2.fastq.gz",
        reads_3_in = "data/fastq_renamed/{id}_2.1.fastq.gz",
        reads_4_in = "data/fastq_renamed/{id}_2.2.fastq.gz"
    output:
        bam1 = "data/aligned_output/align/{id}.sorted.bam",
        bam2 = "data/aligned_output/align/{id}.sorted.bam"
        
    shell:
        """
        bwa mem {setup.reference_index} {input.reads_1_in} {input.reads_2_in} | samtools view -F 4 -Sb | samtools sort -o {output.bam} && samtools index {output.bam1}
        bwa mem {setup.reference_index} {input.reads_3_in} {input.reads_4_in} | samtools view -F 4 -Sb | samtools sort -o {output.bam} && samtools index {output.bam2}
		"""

rule ivar_trim:
    message:
        """
        =======================================================
        Trim the ARTIC primers with iVar
        =======================================================
        """
    input:
        in1 = "data/aligned_output/align/{id}_1.sorted.bam",
        in2 = "data/aligned_output/align/{id}_2.sorted.bam"
    output:
        out1 = "data/aligned_output/primertrim/{id}_1.sorted.primertrim.bam",
        out2 = "data/aligned_output/primertrim/{id}_2.sorted.primertrim.bam"
    shell:
        """
        ivar trim -i {input.in1} -b {setup.bed_file} -p {output.out1}
        ivar trim -i {input.in2} -b {setup.bed_file} -p {output.out2}
        """

rule sort_bam:
    message:
        """
        =======================================================
        Sort the primer-trimmed file
        =======================================================
        """
    input:
        in1= "data/aligned_output/primertrim/{id}_1.sorted.primertrim.bam",
        in2= "data/aligned_output/primertrim/{id}_2.sorted.primertrim.bam"

    output:
        bam1 = "data/aligned_output/primertrim_sorted/{id}_1.removed.primertrim.sorted.bam",
        bai1 = "data/aligned_output/primertrim_sorted/{id}_.removed.primertrim.sorted.bai",
        bam2 = "data/aligned_output/primertrim_sorted/{id}_1.removed.primertrim.sorted.bam",
        bai2 = "data/aligned_output/primertrim_sorted/{id}_.removed.primertrim.sorted.bai"

    shell:
        """
        PicardCommandLine SortSam SO=coordinate INPUT={input.in1} OUTPUT={output.bam1} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true
        PicardCommandLine SortSam SO=coordinate INPUT={input.in2} OUTPUT={output.bam2} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true
        """

rule get_coverage:
    message:
        """
        =======================================================
        Get coverage with samtools
        =======================================================
        """
    input:
        in_1= "data/aligned_output/primertrim_sorted/{id}_1.removed.primertrim.sorted.bam",
        in_2= "data/aligned_output/primertrim_sorted/{id}_2.removed.primertrim.sorted.bam"
    output:
        out_1="data/ivar_output/coverage/{id}_1.coverage.csv",
        out_2="data/ivar_output/coverage/{id}_2.coverage.csv"
    shell:
        """
        samtools depth -a -d 100000 {input.in_1} > {output.out_1}
        samtools depth -a -d 100000 {input.in_2} > {output.out_2}
        """
        
rule get_consensus:
    message:
        """
        =======================================================
        Get the consensus sequence with iVar
        =======================================================
        """
    input:
        bam_file1 = "data/aligned_output/primertrim_sorted/{id}_1.removed.primertrim.sorted.bam",
        bam_file2 = "data/aligned_output/primertrim_sorted/{id}_2.removed.primertrim.sorted.bam"

    output:
        consensus_file = "data/ivar_output/consensus/{id}.fa",
        merged_bam = "data/ivar_output/consensus/{id}_merged.bam"
    shell:
        """
        samtools merge {output._merged_bam} {input.bam_file1} {input.bam_file2} 
    	samtools mpileup -a -A -d 100000 -Q 0 --reference {setup.reference_fasta} {output.merged.bam} | ivar consensus -p {output.consensus_file} -n N -q {setup.min_qual_score} -t {setup.consensus_threshold} -m {setup.min_depth}"

rule combine_and_export:
    message:
        """
        =======================================================
        Combine into a single fasta 
        =======================================================
        """
    input:
        consensus_files = expand("data/ivar_output/consensus/{id}.fa", id = IDS),
    output:
        "all.consensus.fasta",
        
    shell:
        """
        awk 'NR == 1 || FNR > 1'  {input}  >  {output}
        """